{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foZP7l_BlQPa",
        "outputId": "844b02c1-8837-4dfa-f852-206253eec589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in ./.venv/lib/python3.13/site-packages (1.65.5)\n",
            "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (2.32.3)\n",
            "Requirement already satisfied: serpapi in ./.venv/lib/python3.13/site-packages (0.1.5)\n",
            "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.13/site-packages (4.13.3)\n",
            "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.13/site-packages (1.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.13/site-packages (from openai) (4.8.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.13/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.13/site-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.13/site-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in ./.venv/lib/python3.13/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.13/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.13/site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.13/site-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install openai requests serpapi beautifulsoup4 python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3iiI-N0l4Pi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating course for: course on robotics\n",
            "Scraping content from the web...\n",
            "Scraping article: https://www.coursera.org/courses?query=robotics\n",
            "Scraping article: https://www.edx.org/learn/robotics\n",
            "Scraping article: https://robotics.umich.edu/academics/courses/online-courses/\n",
            "Generating course structure using GPT-4...\n",
            "\n",
            "Generated Course Content:\n",
            "\n",
            "Error occurred while generating course content: \n",
            "\n",
            "You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
            "\n",
            "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
            "\n",
            "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
            "\n",
            "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "import requests\n",
        "from serpapi import GoogleSearch\n",
        "from bs4 import BeautifulSoup\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Set up API keys for OpenAI and SerpAPI\n",
        "openai.api_key = \"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\"\n",
        "serpapi_api_key = \"fff4ef49c338ca20f7f70fee5cff4aa15e2fb06604b32f8067d2e8ad7a463f5a\"\n",
        "\n",
        "# Check if the API keys are set\n",
        "if not openai.api_key:\n",
        "    raise ValueError(\"OPENAI_API_KEY is missing. Please set it in your .env file.\")\n",
        "if not serpapi_api_key:\n",
        "    raise ValueError(\"SERPAPI_API_KEY is missing. Please set it in your .env file.\")\n",
        "\n",
        "# Helper Class: EnhancedResearchAgent for scraping content from Google\n",
        "class EnhancedResearchAgent:\n",
        "    def __init__(self, serpapi_key):\n",
        "        self.serpapi_key = serpapi_key\n",
        "\n",
        "    def get_full_content(self, url):\n",
        "        \"\"\"\n",
        "        Extracts full content from a given URL, cleaning up unnecessary parts.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Extract article body (assuming articles are in <article> or <div class=\"article-content\">)\n",
        "            article = soup.find(['article', 'div'], class_='article-content')\n",
        "            if not article:\n",
        "                # If no article class is found, try all <p> tags\n",
        "                paragraphs = soup.find_all('p')\n",
        "                content = \" \".join([para.text for para in paragraphs])\n",
        "            else:\n",
        "                # If found, extract article paragraphs\n",
        "                paragraphs = article.find_all('p')\n",
        "                content = \" \".join([para.text for para in paragraphs])\n",
        "\n",
        "            # Clean and return content\n",
        "            return self.clean_content(content)\n",
        "        except Exception as e:\n",
        "            print(f\"Error while scraping content from {url}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def clean_content(self, content):\n",
        "        \"\"\"\n",
        "        Cleans the scraped content to remove unnecessary white spaces and non-relevant text.\n",
        "        \"\"\"\n",
        "        cleaned_content = content.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
        "        cleaned_content = \" \".join(cleaned_content.split())\n",
        "        return cleaned_content\n",
        "\n",
        "    def web_scrape(self, query: str):\n",
        "        \"\"\"\n",
        "        Scrapes web data using SerpAPI and extracts full content from each top result.\n",
        "        \"\"\"\n",
        "        params = {\n",
        "            \"q\": query,\n",
        "            \"api_key\": self.serpapi_key,\n",
        "            \"engine\": \"google\",\n",
        "            \"num\": 5,  # Retrieve top 5 search results\n",
        "        }\n",
        "        search = GoogleSearch(params)\n",
        "        results = search.get_dict()\n",
        "\n",
        "        full_content = []\n",
        "        for result in results.get(\"organic_results\", []):\n",
        "            if \"link\" in result:\n",
        "                article_url = result[\"link\"]\n",
        "                print(f\"Scraping article: {article_url}\")\n",
        "                content = self.get_full_content(article_url)\n",
        "                if content:\n",
        "                    full_content.append(content)\n",
        "\n",
        "        return full_content\n",
        "\n",
        "# Class: Content Generation Agent for course creation using GPT-4\n",
        "class ContentGenerationAgent:\n",
        "    def __init__(self, subject_name: str, research_agent: EnhancedResearchAgent):\n",
        "        self.subject_name = subject_name\n",
        "        self.research_agent = research_agent\n",
        "\n",
        "    def generate_course(self, course_description: str):\n",
        "        \"\"\"\n",
        "        Generates a structured course using scraped data and GPT-4.\n",
        "        \"\"\"\n",
        "        print(f\"Generating course for: {course_description}\")\n",
        "\n",
        "        # Step 1: Use the research agent to scrape relevant articles\n",
        "        print(\"Scraping content from the web...\")\n",
        "        scraped_data = self.research_agent.web_scrape(course_description)\n",
        "\n",
        "        # Step 2: Process the scraped data using GPT-4 to generate course modules\n",
        "        print(\"Generating course structure using GPT-4...\")\n",
        "        prompt = f\"\"\"\n",
        "        Based on the following information, create a comprehensive 6-week course:\n",
        "        Course Description: {course_description}\n",
        "        Scraped Data: {scraped_data}\n",
        "\n",
        "        Generate 5-6 modules, with detailed lessons, examples, and resources.\n",
        "        Each module should contain a title, lessons, and relevant examples.\n",
        "        The course should be beginner-friendly and educational.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = openai.Completion.create(\n",
        "                model=\"gpt-4\",  # Use GPT-4 to process and generate course\n",
        "                prompt=prompt,\n",
        "                max_tokens=1500,\n",
        "                temperature=0.7,\n",
        "                n=1,\n",
        "                stop=None,\n",
        "            )\n",
        "\n",
        "            course_data = response.choices[0].text.strip()\n",
        "            return course_data\n",
        "        except Exception as e:\n",
        "            return f\"Error occurred while generating course content: {str(e)}\"\n",
        "\n",
        "# Main function to drive the system\n",
        "def main():\n",
        "    # Take input for course description\n",
        "    course_description = input(\"Enter the course description: \")\n",
        "\n",
        "    # Initialize the Research and Content Generation agents\n",
        "    research_agent = EnhancedResearchAgent(serpapi_api_key)\n",
        "    content_generation_agent = ContentGenerationAgent(course_description, research_agent)\n",
        "\n",
        "    # Generate the course\n",
        "    course_output = content_generation_agent.generate_course(course_description)\n",
        "\n",
        "    print(\"\\nGenerated Course Content:\\n\")\n",
        "    print(course_output)\n",
        "\n",
        "# Entry point of the program\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
